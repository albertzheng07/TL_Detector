{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trafic Light Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azheng/anaconda3/envs/carnd-term1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/azheng/anaconda3/envs/carnd-term1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/azheng/anaconda3/envs/carnd-term1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/azheng/anaconda3/envs/carnd-term1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd23295160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "x_label = []\n",
    "for img_class, directory in enumerate(['red', 'yellow', 'green', 'none']):\n",
    "    for i, file_name in enumerate(glob.glob(\"{}/*.jpg\".format(directory))):\n",
    "        file = cv2.imread(file_name)\n",
    "\n",
    "        file = cv2.cvtColor(file, cv2.COLOR_BGR2RGB);\n",
    "        resized = cv2.resize(file, (32,64))\n",
    "\n",
    "        X_train.append(resized/255.)\n",
    "        x_label.append(img_class)\n",
    "        \n",
    "        #if (i < 3):\n",
    "        #    plt.imshow(rgb)\n",
    "        #    plt.show()\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "x_label = np.array(x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import losses, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_labels = to_categorical(x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 32, 3), padding='same', activation='relu', kernel_initializer='random_uniform', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "Dropout(0.8)\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='random_uniform', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "Dropout(0.8)\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.add(Dense(128, activation='relu', kernel_initializer='random_uniform', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='random_uniform', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.categorical_crossentropy\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 330 samples, validate on 37 samples\n",
      "Epoch 1/10\n",
      "330/330 [==============================] - 0s - loss: 1.6448 - acc: 0.3424 - val_loss: 1.8597 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "330/330 [==============================] - 0s - loss: 1.3651 - acc: 0.4727 - val_loss: 2.4655 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "330/330 [==============================] - 0s - loss: 1.0544 - acc: 0.6939 - val_loss: 2.3145 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "330/330 [==============================] - 0s - loss: 0.8383 - acc: 0.7455 - val_loss: 2.0328 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "330/330 [==============================] - 0s - loss: 0.6622 - acc: 0.8061 - val_loss: 1.9313 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "330/330 [==============================] - 0s - loss: 0.5628 - acc: 0.8848 - val_loss: 1.9637 - val_acc: 0.0270\n",
      "Epoch 7/10\n",
      "330/330 [==============================] - 0s - loss: 0.4777 - acc: 0.9061 - val_loss: 1.9638 - val_acc: 0.1081\n",
      "Epoch 8/10\n",
      "330/330 [==============================] - 0s - loss: 0.3871 - acc: 0.9515 - val_loss: 1.9632 - val_acc: 0.3243\n",
      "Epoch 9/10\n",
      "330/330 [==============================] - 0s - loss: 0.3496 - acc: 0.9485 - val_loss: 2.2054 - val_acc: 0.1081\n",
      "Epoch 10/10\n",
      "330/330 [==============================] - 0s - loss: 0.3243 - acc: 0.9697 - val_loss: 2.3170 - val_acc: 0.08110.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd2468ea20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, categorical_labels, batch_size=32, epochs=10, verbose=True, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_train, categorical_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5029275748645252, 0.8746594008291775]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/367 [=========================>....] - ETA: 0s0 [9.9670058e-01 1.3424919e-04 5.3665346e-05 3.1115019e-03] [1. 0. 0. 0.]\n",
      "1 [0.9711586  0.00458834 0.00112115 0.02313187] [1. 0. 0. 0.]\n",
      "2 [9.9057746e-01 3.3716488e-04 2.1010428e-05 9.0644248e-03] [1. 0. 0. 0.]\n",
      "3 [9.9528342e-01 4.2881680e-04 6.3563166e-05 4.2241951e-03] [1. 0. 0. 0.]\n",
      "4 [9.7014284e-01 3.1841109e-03 6.7046960e-05 2.6605941e-02] [1. 0. 0. 0.]\n",
      "5 [9.88383114e-01 1.16711750e-03 8.78205719e-06 1.04408935e-02] [1. 0. 0. 0.]\n",
      "6 [0.37219888 0.11582977 0.23956813 0.27240318] [1. 0. 0. 0.]\n",
      "7 [0.730309   0.08252031 0.09010472 0.09706597] [1. 0. 0. 0.]\n",
      "8 [0.5578431  0.09119952 0.12145755 0.22949985] [1. 0. 0. 0.]\n",
      "9 [0.5509352  0.07576776 0.11326472 0.26003233] [1. 0. 0. 0.]\n",
      "10 [9.9725926e-01 3.5878603e-04 4.5365639e-04 1.9282234e-03] [1. 0. 0. 0.]\n",
      "11 [9.6372819e-01 4.7318949e-04 1.0647882e-02 2.5150733e-02] [1. 0. 0. 0.]\n",
      "12 [0.9855672  0.00108001 0.00792627 0.00542649] [1. 0. 0. 0.]\n",
      "13 [9.7501695e-01 3.6371159e-04 2.1237925e-02 3.3813743e-03] [1. 0. 0. 0.]\n",
      "14 [9.9951744e-01 3.6490717e-05 2.2515138e-05 4.2361257e-04] [1. 0. 0. 0.]\n",
      "15 [9.9597943e-01 1.0210174e-03 1.8547486e-05 2.9809661e-03] [1. 0. 0. 0.]\n",
      "16 [9.9900490e-01 4.2645284e-04 4.7378426e-06 5.6384620e-04] [1. 0. 0. 0.]\n",
      "17 [0.9413479  0.04178045 0.00626847 0.01060318] [1. 0. 0. 0.]\n",
      "18 [9.9081731e-01 4.6952142e-04 4.1082156e-05 8.6721135e-03] [1. 0. 0. 0.]\n",
      "19 [9.9464053e-01 1.1677345e-03 3.8981671e-05 4.1527268e-03] [1. 0. 0. 0.]\n",
      "20 [9.9927968e-01 2.3474115e-05 4.8581314e-05 6.4822176e-04] [1. 0. 0. 0.]\n",
      "21 [9.8560494e-01 6.5646140e-04 8.0387501e-05 1.3658267e-02] [1. 0. 0. 0.]\n",
      "22 [9.9422628e-01 7.3408178e-04 6.5166016e-05 4.9745408e-03] [1. 0. 0. 0.]\n",
      "23 [9.9606401e-01 7.8881305e-05 2.2991086e-05 3.8340979e-03] [1. 0. 0. 0.]\n",
      "24 [9.9606401e-01 7.8881305e-05 2.2991086e-05 3.8340979e-03] [1. 0. 0. 0.]\n",
      "25 [9.7099227e-01 1.2968014e-03 2.9018594e-04 2.7420698e-02] [1. 0. 0. 0.]\n",
      "26 [9.9483192e-01 8.5344858e-04 2.5526841e-05 4.2890874e-03] [1. 0. 0. 0.]\n",
      "27 [9.9836046e-01 3.8068396e-05 3.5280196e-04 1.2487053e-03] [1. 0. 0. 0.]\n",
      "28 [9.8650509e-01 1.8659742e-03 8.7314853e-05 1.1541693e-02] [1. 0. 0. 0.]\n",
      "29 [9.987606e-01 1.318343e-05 6.289763e-05 1.163365e-03] [1. 0. 0. 0.]\n",
      "30 [0.9789169  0.01082724 0.00394602 0.00630981] [1. 0. 0. 0.]\n",
      "31 [9.9789494e-01 4.0659492e-04 1.2347029e-05 1.6860825e-03] [1. 0. 0. 0.]\n",
      "32 [9.9661523e-01 4.7008638e-04 4.7134086e-05 2.8674691e-03] [1. 0. 0. 0.]\n",
      "33 [9.8814917e-01 1.0098296e-03 2.1698962e-04 1.0623969e-02] [1. 0. 0. 0.]\n",
      "34 [9.9952769e-01 1.2678944e-04 1.2069828e-05 3.3348484e-04] [1. 0. 0. 0.]\n",
      "35 [9.9732840e-01 6.9190020e-04 1.0603102e-05 1.9690881e-03] [1. 0. 0. 0.]\n",
      "36 [9.9813062e-01 1.6117300e-04 2.0262777e-04 1.5055813e-03] [1. 0. 0. 0.]\n",
      "37 [9.9840742e-01 3.1976771e-04 9.2315076e-06 1.2635231e-03] [1. 0. 0. 0.]\n",
      "38 [9.9739397e-01 8.3077326e-04 2.1749918e-04 1.5578225e-03] [1. 0. 0. 0.]\n",
      "39 [0.9794484  0.01047299 0.00384933 0.00622931] [1. 0. 0. 0.]\n",
      "40 [9.9599957e-01 7.9542151e-05 2.3389945e-05 3.8975528e-03] [1. 0. 0. 0.]\n",
      "41 [9.8893332e-01 2.4553679e-05 2.0537441e-04 1.0836807e-02] [1. 0. 0. 0.]\n",
      "42 [9.8039436e-01 2.1310938e-03 1.4055417e-05 1.7460501e-02] [1. 0. 0. 0.]\n",
      "43 [9.9620867e-01 2.8248809e-04 5.4648459e-05 3.4542398e-03] [1. 0. 0. 0.]\n",
      "44 [9.9673706e-01 9.9882332e-04 1.5648373e-05 2.2483610e-03] [1. 0. 0. 0.]\n",
      "45 [9.3506664e-01 1.3510358e-02 1.1326754e-04 5.1309790e-02] [1. 0. 0. 0.]\n",
      "46 [9.93358433e-01 4.66503443e-05 1.09055276e-04 6.48587383e-03] [1. 0. 0. 0.]\n",
      "47 [9.9606401e-01 7.8881305e-05 2.2991086e-05 3.8340979e-03] [1. 0. 0. 0.]\n",
      "48 [9.0553594e-01 6.0554344e-04 9.1555584e-03 8.4702961e-02] [1. 0. 0. 0.]\n",
      "49 [9.9651271e-01 5.4170901e-04 2.5637044e-05 2.9199803e-03] [1. 0. 0. 0.]\n",
      "50 [9.9929285e-01 1.1014472e-05 3.0122848e-05 6.6596805e-04] [1. 0. 0. 0.]\n",
      "51 [9.9084258e-01 3.0609875e-04 4.3673990e-05 8.8076321e-03] [1. 0. 0. 0.]\n",
      "52 [9.9773079e-01 3.7415186e-04 1.8945693e-05 1.8761677e-03] [1. 0. 0. 0.]\n",
      "53 [0.97897947 0.01081425 0.00392523 0.00628101] [1. 0. 0. 0.]\n",
      "54 [9.9877292e-01 4.3739413e-04 7.0727915e-06 7.8261801e-04] [1. 0. 0. 0.]\n",
      "55 [9.9246430e-01 2.3481894e-04 2.5949461e-04 7.0413980e-03] [1. 0. 0. 0.]\n",
      "56 [9.98918653e-01 1.14903574e-04 1.18257012e-05 9.54711810e-04] [1. 0. 0. 0.]\n",
      "57 [0.97316056 0.00193963 0.00388086 0.02101897] [1. 0. 0. 0.]\n",
      "58 [9.9701309e-01 6.2143465e-04 1.4227480e-05 2.3512291e-03] [1. 0. 0. 0.]\n",
      "59 [9.9839264e-01 1.5964860e-04 9.9253026e-05 1.3483783e-03] [1. 0. 0. 0.]\n",
      "60 [0.6827571  0.05372702 0.03217851 0.2313374 ] [1. 0. 0. 0.]\n",
      "61 [9.9431133e-01 1.5638520e-04 4.6692559e-05 5.4856278e-03] [1. 0. 0. 0.]\n",
      "62 [9.99023914e-01 2.69214343e-05 1.18636446e-04 8.30544101e-04] [1. 0. 0. 0.]\n",
      "63 [0.97621095 0.01260874 0.0054378  0.00574257] [1. 0. 0. 0.]\n",
      "64 [9.9926323e-01 6.4860624e-06 3.1662377e-04 4.1364701e-04] [1. 0. 0. 0.]\n",
      "65 [9.9606979e-01 7.8686789e-05 2.2958242e-05 3.8286464e-03] [1. 0. 0. 0.]\n",
      "66 [0.91399395 0.03895449 0.00096896 0.04608264] [1. 0. 0. 0.]\n",
      "67 [9.9207413e-01 3.4965909e-05 1.4390306e-04 7.7469619e-03] [1. 0. 0. 0.]\n",
      "68 [9.7240889e-01 1.0575737e-02 5.4059585e-04 1.6474741e-02] [1. 0. 0. 0.]\n",
      "69 [9.9827361e-01 7.4301258e-04 5.8954924e-06 9.7738730e-04] [1. 0. 0. 0.]\n",
      "70 [0.97944176 0.01046765 0.00385538 0.00623519] [1. 0. 0. 0.]\n",
      "71 [9.9606401e-01 7.8881305e-05 2.2991086e-05 3.8340979e-03] [1. 0. 0. 0.]\n",
      "72 [9.9908996e-01 2.2666031e-04 1.2781728e-05 6.7060592e-04] [1. 0. 0. 0.]\n",
      "73 [9.9581015e-01 8.3387727e-05 2.3779150e-05 4.0826923e-03] [1. 0. 0. 0.]\n",
      "74 [0.89560795 0.00314298 0.00253787 0.09871117] [1. 0. 0. 0.]\n",
      "75 [0.8737855  0.01709236 0.04214903 0.06697313] [1. 0. 0. 0.]\n",
      "76 [9.9803704e-01 7.8375677e-05 3.0064057e-05 1.8545393e-03] [1. 0. 0. 0.]\n",
      "77 [9.9606401e-01 7.8881305e-05 2.2991086e-05 3.8340979e-03] [1. 0. 0. 0.]\n",
      "78 [9.9978715e-01 3.5406185e-05 1.2360159e-05 1.6502198e-04] [1. 0. 0. 0.]\n",
      "79 [9.9641514e-01 3.0193559e-03 7.9140664e-06 5.5752340e-04] [1. 0. 0. 0.]\n",
      "80 [9.9895227e-01 3.3935055e-04 7.3385813e-06 7.0100138e-04] [1. 0. 0. 0.]\n",
      "81 [9.9542844e-01 6.1974616e-04 5.4086082e-05 3.8976555e-03] [1. 0. 0. 0.]\n",
      "82 [0.762231   0.01002843 0.01788658 0.209854  ] [1. 0. 0. 0.]\n",
      "83 [9.9987304e-01 6.9215689e-06 4.5624124e-06 1.1547749e-04] [1. 0. 0. 0.]\n",
      "84 [9.8250985e-01 1.0644197e-03 7.9583973e-05 1.6346259e-02] [1. 0. 0. 0.]\n",
      "85 [9.6282190e-01 1.3270655e-04 3.1993720e-03 3.3845924e-02] [1. 0. 0. 0.]\n",
      "86 [9.9797732e-01 8.1997673e-04 1.1500441e-05 1.1912172e-03] [1. 0. 0. 0.]\n",
      "87 [9.9701488e-01 1.4431848e-03 7.8504863e-06 1.5340926e-03] [1. 0. 0. 0.]\n",
      "88 [9.7024643e-01 8.6510432e-04 2.2446180e-02 6.4423513e-03] [1. 0. 0. 0.]\n",
      "89 [9.9977762e-01 2.6316122e-06 3.3218585e-05 1.8653132e-04] [1. 0. 0. 0.]\n",
      "90 [9.9903786e-01 3.8678518e-05 2.9490984e-04 6.2859408e-04] [1. 0. 0. 0.]\n",
      "91 [9.9578899e-01 1.8994681e-03 4.5864534e-05 2.2656918e-03] [1. 0. 0. 0.]\n",
      "92 [9.9552923e-01 1.5847328e-03 2.7526158e-04 2.6107763e-03] [1. 0. 0. 0.]\n",
      "93 [9.81159568e-01 1.28680365e-02 7.53352942e-05 5.89706842e-03] [1. 0. 0. 0.]\n",
      "94 [0.8826283  0.0797135  0.00359289 0.0340653 ] [1. 0. 0. 0.]\n",
      "95 [0.96094406 0.0247582  0.0021807  0.01211712] [1. 0. 0. 0.]\n",
      "96 [0.97271293 0.00198073 0.00166847 0.02363787] [1. 0. 0. 0.]\n",
      "97 [0.01309232 0.9724501  0.00666336 0.00779426] [0. 1. 0. 0.]\n",
      "98 [1.7123477e-03 9.9539703e-01 9.1782416e-04 1.9726958e-03] [0. 1. 0. 0.]\n",
      "99 [1.3408950e-03 9.8080277e-01 6.1103585e-04 1.7245362e-02] [0. 1. 0. 0.]\n",
      "100 [0.0067098  0.9857615  0.00251745 0.00501123] [0. 1. 0. 0.]\n",
      "101 [7.2893832e-04 9.9855274e-01 1.4908786e-04 5.6927919e-04] [0. 1. 0. 0.]\n",
      "102 [2.3634672e-04 9.7278589e-01 3.6783994e-03 2.3299366e-02] [0. 1. 0. 0.]\n",
      "103 [2.3711254e-04 9.7277588e-01 3.6737095e-03 2.3313351e-02] [0. 1. 0. 0.]\n",
      "104 [2.3693906e-04 9.7274482e-01 3.6850257e-03 2.3333188e-02] [0. 1. 0. 0.]\n",
      "105 [2.3719898e-04 9.7278905e-01 3.6705758e-03 2.3303147e-02] [0. 1. 0. 0.]\n",
      "106 [2.3731899e-04 9.7278386e-01 3.6709956e-03 2.3307834e-02] [0. 1. 0. 0.]\n",
      "107 [2.3711254e-04 9.7277588e-01 3.6737095e-03 2.3313351e-02] [0. 1. 0. 0.]\n",
      "108 [2.3693906e-04 9.7274482e-01 3.6850257e-03 2.3333188e-02] [0. 1. 0. 0.]\n",
      "109 [5.1559292e-02 9.4252849e-01 2.0031803e-04 5.7119499e-03] [0. 1. 0. 0.]\n",
      "110 [0.15541443 0.61658967 0.03127464 0.19672129] [0. 1. 0. 0.]\n",
      "111 [0.03194274 0.91983175 0.00847771 0.03974782] [0. 1. 0. 0.]\n",
      "112 [0.02301239 0.9376277  0.01048064 0.02887922] [0. 1. 0. 0.]\n",
      "113 [0.00715779 0.9709165  0.00644174 0.01548403] [0. 1. 0. 0.]\n",
      "114 [1.11926064e-01 8.87991011e-01 1.18041994e-08 8.28921402e-05] [0. 1. 0. 0.]\n",
      "115 [0.0074261  0.9516052  0.01721474 0.02375406] [0. 1. 0. 0.]\n",
      "116 [0.01078553 0.9368371  0.02138662 0.03099075] [0. 1. 0. 0.]\n",
      "117 [0.01023389 0.9229211  0.03779662 0.02904837] [0. 1. 0. 0.]\n",
      "118 [0.03051488 0.8202508  0.09456728 0.054667  ] [0. 1. 0. 0.]\n",
      "119 [0.01105774 0.9389467  0.02115216 0.0288434 ] [0. 1. 0. 0.]\n",
      "120 [0.00521135 0.96428806 0.0120855  0.01841507] [0. 1. 0. 0.]\n",
      "121 [0.09061123 0.5817621  0.1701077  0.15751904] [0. 1. 0. 0.]\n",
      "122 [0.01491385 0.8296137  0.14152712 0.01394534] [0. 1. 0. 0.]\n",
      "123 [3.7086618e-05 9.9371028e-01 2.2444457e-03 4.0082419e-03] [0. 1. 0. 0.]\n",
      "124 [0.00151281 0.9960466  0.00124256 0.0011981 ] [0. 1. 0. 0.]\n",
      "125 [1.1526328e-04 9.8309946e-01 6.8155746e-03 9.9696713e-03] [0. 1. 0. 0.]\n",
      "126 [0.00621845 0.9774508  0.01245863 0.00387214] [0. 1. 0. 0.]\n",
      "127 [0.01406104 0.9632191  0.01515956 0.00756034] [0. 1. 0. 0.]\n",
      "128 [0.0034414  0.9589463  0.00380883 0.03380349] [0. 1. 0. 0.]\n",
      "129 [0.00577025 0.9879511  0.00288883 0.00338972] [0. 1. 0. 0.]\n",
      "130 [1.4913024e-04 9.9943119e-01 1.1414100e-04 3.0542744e-04] [0. 1. 0. 0.]\n",
      "131 [3.0564348e-04 9.9359834e-01 1.3805827e-04 5.9579266e-03] [0. 1. 0. 0.]\n",
      "132 [7.7329918e-05 9.8523325e-01 3.2198639e-03 1.1469545e-02] [0. 1. 0. 0.]\n",
      "133 [0.00943826 0.98057175 0.00485357 0.00513645] [0. 1. 0. 0.]\n",
      "134 [2.0362042e-04 9.9973899e-01 8.4412920e-07 5.6488585e-05] [0. 1. 0. 0.]\n",
      "135 [6.2557980e-03 9.9031854e-01 6.9231330e-04 2.7333102e-03] [0. 1. 0. 0.]\n",
      "136 [0.00656445 0.97144926 0.01496883 0.00701746] [0. 1. 0. 0.]\n",
      "137 [0.00575736 0.98924327 0.00127091 0.00372848] [0. 1. 0. 0.]\n",
      "138 [0.0285315  0.9498365  0.00980693 0.01182514] [0. 1. 0. 0.]\n",
      "139 [0.00386065 0.99194545 0.00120633 0.00298753] [0. 1. 0. 0.]\n",
      "140 [0.00307524 0.98725057 0.00582673 0.00384734] [0. 1. 0. 0.]\n",
      "141 [4.4063307e-04 9.9603844e-01 7.3032570e-05 3.4478707e-03] [0. 1. 0. 0.]\n",
      "142 [1.2955425e-04 9.9944454e-01 9.8676865e-05 3.2711850e-04] [0. 1. 0. 0.]\n",
      "143 [4.4238484e-03 9.9545950e-01 6.3019240e-07 1.1593633e-04] [0. 1. 0. 0.]\n",
      "144 [4.5790544e-04 9.8106295e-01 1.2166941e-03 1.7262427e-02] [0. 1. 0. 0.]\n",
      "145 [3.313525e-05 9.910745e-01 2.158055e-03 6.734304e-03] [0. 1. 0. 0.]\n",
      "146 [4.2600784e-04 9.8012275e-01 2.7147522e-03 1.6736481e-02] [0. 1. 0. 0.]\n",
      "147 [8.58366548e-05 9.99787748e-01 1.10852779e-05 1.15357456e-04] [0. 1. 0. 0.]\n",
      "148 [0.03077591 0.48245522 0.4640668  0.02270209] [0. 1. 0. 0.]\n",
      "149 [0.00406144 0.97663295 0.01427014 0.00503546] [0. 1. 0. 0.]\n",
      "150 [0.03578101 0.88470584 0.06892505 0.01058821] [0. 1. 0. 0.]\n",
      "151 [0.006536   0.98291135 0.00520505 0.00534765] [0. 1. 0. 0.]\n",
      "152 [1.7741858e-03 9.9713957e-01 9.6678334e-05 9.8962639e-04] [0. 1. 0. 0.]\n",
      "153 [0.00318618 0.9868153  0.00515693 0.00484163] [0. 1. 0. 0.]\n",
      "154 [1.1571755e-03 9.9801874e-01 2.7325415e-04 5.5084267e-04] [0. 1. 0. 0.]\n",
      "155 [0.00723396 0.9772748  0.01080261 0.0046887 ] [0. 1. 0. 0.]\n",
      "156 [0.04314388 0.8524516  0.09089036 0.01351416] [0. 1. 0. 0.]\n",
      "157 [2.2667219e-04 9.9759030e-01 9.7400026e-04 1.2090337e-03] [0. 1. 0. 0.]\n",
      "158 [0.01208866 0.961043   0.01464868 0.01221963] [0. 1. 0. 0.]\n",
      "159 [2.6419463e-03 9.9543083e-01 5.0917058e-04 1.4179841e-03] [0. 1. 0. 0.]\n",
      "160 [5.4833876e-05 9.9122733e-01 1.6977261e-03 7.0201112e-03] [0. 1. 0. 0.]\n",
      "161 [2.1212623e-05 9.9499786e-01 1.3259104e-03 3.6550676e-03] [0. 1. 0. 0.]\n",
      "162 [0.01325535 0.9733479  0.00511204 0.0082847 ] [0. 1. 0. 0.]\n",
      "163 [0.00370266 0.9757875  0.01470523 0.00580465] [0. 1. 0. 0.]\n",
      "164 [0.00522503 0.98522574 0.00413517 0.00541406] [0. 1. 0. 0.]\n",
      "165 [5.8693963e-04 9.9496907e-01 2.6762325e-03 1.7677873e-03] [0. 1. 0. 0.]\n",
      "166 [2.1628171e-04 9.9970323e-01 3.0701917e-06 7.7319688e-05] [0. 1. 0. 0.]\n",
      "167 [1.5151263e-03 9.9840027e-01 2.3938683e-06 8.2249448e-05] [0. 1. 0. 0.]\n",
      "168 [0.09289167 0.873467   0.01064727 0.02299404] [0. 1. 0. 0.]\n",
      "169 [0.07777917 0.8973445  0.0071983  0.01767808] [0. 1. 0. 0.]\n",
      "170 [0.4472674  0.5310989  0.00422664 0.01740708] [0. 1. 0. 0.]\n",
      "171 [0.00987594 0.88584137 0.05912105 0.04516162] [0. 1. 0. 0.]\n",
      "172 [3.2842057e-04 9.9750155e-01 1.0010293e-04 2.0699264e-03] [0. 1. 0. 0.]\n",
      "173 [2.0596402e-02 9.7004259e-01 4.4497760e-04 8.9160670e-03] [0. 1. 0. 0.]\n",
      "174 [1.0033834e-03 9.9706358e-01 8.4519583e-05 1.8484814e-03] [0. 1. 0. 0.]\n",
      "175 [3.7760958e-01 6.1851341e-01 4.7679507e-05 3.8293682e-03] [0. 1. 0. 0.]\n",
      "176 [8.9566507e-05 9.9960512e-01 7.8339253e-06 2.9752468e-04] [0. 1. 0. 0.]\n",
      "177 [8.4970248e-05 9.9941802e-01 1.5758469e-05 4.8131027e-04] [0. 1. 0. 0.]\n",
      "178 [4.378722e-06 6.250880e-05 9.952602e-01 4.672914e-03] [0. 0. 1. 0.]\n",
      "179 [1.6869114e-06 1.4853750e-05 9.9877709e-01 1.2063941e-03] [0. 0. 1. 0.]\n",
      "180 [1.0817079e-06 1.1500125e-04 9.9805593e-01 1.8279700e-03] [0. 0. 1. 0.]\n",
      "181 [2.4484840e-05 8.5572503e-04 9.9611938e-01 3.0005102e-03] [0. 0. 1. 0.]\n",
      "182 [7.389632e-04 7.376433e-05 9.954953e-01 3.691896e-03] [0. 0. 1. 0.]\n",
      "183 [0.03255591 0.04093334 0.28105876 0.64545196] [0. 0. 1. 0.]\n",
      "184 [0.00305409 0.022293   0.8168218  0.15783113] [0. 0. 1. 0.]\n",
      "185 [4.3791335e-04 6.0493015e-03 9.8105717e-01 1.2455690e-02] [0. 0. 1. 0.]\n",
      "186 [2.6369924e-03 4.2576558e-04 6.9339508e-01 3.0354211e-01] [0. 0. 1. 0.]\n",
      "187 [0.01039033 0.00280541 0.94385207 0.04295218] [0. 0. 1. 0.]\n",
      "188 [0.00960565 0.02713507 0.9083811  0.05487821] [0. 0. 1. 0.]\n",
      "189 [0.01387826 0.0244215  0.8845199  0.07718033] [0. 0. 1. 0.]\n",
      "190 [1.7580952e-04 8.4424838e-03 9.8511744e-01 6.2643224e-03] [0. 0. 1. 0.]\n",
      "191 [2.2027378e-03 8.8294432e-04 9.6420330e-01 3.2711051e-02] [0. 0. 1. 0.]\n",
      "192 [3.9242124e-04 3.8041334e-04 9.8329562e-01 1.5931537e-02] [0. 0. 1. 0.]\n",
      "193 [4.9869716e-04 7.4816314e-03 9.4734710e-01 4.4672564e-02] [0. 0. 1. 0.]\n",
      "194 [0.00306396 0.27361485 0.681721   0.04160023] [0. 0. 1. 0.]\n",
      "195 [0.00096571 0.02581037 0.96250695 0.01071695] [0. 0. 1. 0.]\n",
      "196 [0.00119039 0.02181328 0.9657675  0.01122882] [0. 0. 1. 0.]\n",
      "197 [0.00709464 0.03331657 0.8449422  0.11464655] [0. 0. 1. 0.]\n",
      "198 [0.0116501  0.00689885 0.91817456 0.06327648] [0. 0. 1. 0.]\n",
      "199 [0.00491018 0.00849447 0.9486789  0.03791641] [0. 0. 1. 0.]\n",
      "200 [0.01986676 0.01430113 0.8571917  0.10864049] [0. 0. 1. 0.]\n",
      "201 [0.07462055 0.01547771 0.5787008  0.33120096] [0. 0. 1. 0.]\n",
      "202 [0.02274523 0.74659705 0.21641769 0.01424005] [0. 0. 1. 0.]\n",
      "203 [0.00775507 0.03363599 0.7870118  0.17159712] [0. 0. 1. 0.]\n",
      "204 [0.00257737 0.00225887 0.9284908  0.0666729 ] [0. 0. 1. 0.]\n",
      "205 [0.00737437 0.01677184 0.791097   0.18475686] [0. 0. 1. 0.]\n",
      "206 [0.00440061 0.02793535 0.82294744 0.14471662] [0. 0. 1. 0.]\n",
      "207 [0.00283028 0.00339812 0.93255365 0.06121796] [0. 0. 1. 0.]\n",
      "208 [3.7680784e-05 6.2980823e-04 9.9587351e-01 3.4591029e-03] [0. 0. 1. 0.]\n",
      "209 [1.9689183e-05 9.2180595e-03 9.5110357e-01 3.9658614e-02] [0. 0. 1. 0.]\n",
      "210 [1.6065557e-06 5.8268802e-06 9.9916041e-01 8.3228230e-04] [0. 0. 1. 0.]\n",
      "211 [9.0128109e-05 1.4100822e-04 9.9485695e-01 4.9119014e-03] [0. 0. 1. 0.]\n",
      "212 [2.8346933e-06 1.2781430e-05 9.9809819e-01 1.8862054e-03] [0. 0. 1. 0.]\n",
      "213 [2.2840401e-02 4.1921958e-04 9.6702486e-01 9.7154267e-03] [0. 0. 1. 0.]\n",
      "214 [3.7619775e-05 1.3029075e-03 9.9445736e-01 4.2021563e-03] [0. 0. 1. 0.]\n",
      "215 [7.5649703e-04 7.5073127e-05 9.9541241e-01 3.7560610e-03] [0. 0. 1. 0.]\n",
      "216 [3.7715276e-05 2.2221287e-03 9.9454123e-01 3.1989955e-03] [0. 0. 1. 0.]\n",
      "217 [1.7164840e-06 1.3890237e-02 9.6194261e-01 2.4165429e-02] [0. 0. 1. 0.]\n",
      "218 [2.4810102e-05 2.4614190e-03 9.9152726e-01 5.9865182e-03] [0. 0. 1. 0.]\n",
      "219 [1.0465120e-01 4.2314793e-04 8.8398373e-01 1.0941882e-02] [0. 0. 1. 0.]\n",
      "220 [4.8780549e-04 3.4816283e-05 9.9726701e-01 2.2104329e-03] [0. 0. 1. 0.]\n",
      "221 [6.5907143e-04 6.7461238e-05 9.9586177e-01 3.4115973e-03] [0. 0. 1. 0.]\n",
      "222 [8.100940e-05 6.489652e-03 9.852195e-01 8.209790e-03] [0. 0. 1. 0.]\n",
      "223 [3.9013514e-05 1.9110901e-04 9.9587655e-01 3.8932532e-03] [0. 0. 1. 0.]\n",
      "224 [1.1239508e-02 1.1759480e-04 9.8085344e-01 7.7894982e-03] [0. 0. 1. 0.]\n",
      "225 [1.7878030e-03 2.8960421e-05 9.9637187e-01 1.8113707e-03] [0. 0. 1. 0.]\n",
      "226 [4.2690230e-05 7.8442749e-03 9.7959280e-01 1.2520163e-02] [0. 0. 1. 0.]\n",
      "227 [4.6296013e-06 8.0637292e-05 9.9559915e-01 4.3155532e-03] [0. 0. 1. 0.]\n",
      "228 [3.6739406e-05 5.8837286e-03 9.8969907e-01 4.3805609e-03] [0. 0. 1. 0.]\n",
      "229 [2.7455055e-06 2.2290107e-04 9.9590611e-01 3.8681834e-03] [0. 0. 1. 0.]\n",
      "230 [1.3035241e-01 8.0895226e-04 8.5526812e-01 1.3570515e-02] [0. 0. 1. 0.]\n",
      "231 [7.68779159e-07 1.03440925e-05 9.99197781e-01 7.91215571e-04] [0. 0. 1. 0.]\n",
      "232 [2.1976423e-06 5.8813857e-06 9.9904126e-01 9.5074641e-04] [0. 0. 1. 0.]\n",
      "233 [4.4676051e-03 4.9605220e-05 9.9254602e-01 2.9368522e-03] [0. 0. 1. 0.]\n",
      "234 [3.3229499e-04 2.9550178e-05 9.9758315e-01 2.0550003e-03] [0. 0. 1. 0.]\n",
      "235 [7.7368923e-06 3.1152379e-03 9.8456478e-01 1.2312272e-02] [0. 0. 1. 0.]\n",
      "236 [4.8615235e-05 2.1564972e-04 9.7700113e-01 2.2734696e-02] [0. 0. 1. 0.]\n",
      "237 [1.8436380e-05 3.1857304e-03 9.9381024e-01 2.9855689e-03] [0. 0. 1. 0.]\n",
      "238 [8.4214218e-05 8.5698406e-04 9.9502289e-01 4.0358827e-03] [0. 0. 1. 0.]\n",
      "239 [6.9785345e-04 7.4318319e-05 9.9566108e-01 3.5666279e-03] [0. 0. 1. 0.]\n",
      "240 [3.5887209e-03 4.3218195e-05 9.9431485e-01 2.0532939e-03] [0. 0. 1. 0.]\n",
      "241 [8.5978839e-04 3.5794837e-05 9.9672782e-01 2.3766451e-03] [0. 0. 1. 0.]\n",
      "242 [1.9529694e-05 6.5571687e-04 9.9654418e-01 2.7806375e-03] [0. 0. 1. 0.]\n",
      "243 [8.9667868e-05 2.6945486e-03 9.9078619e-01 6.4296718e-03] [0. 0. 1. 0.]\n",
      "244 [1.9631332e-05 1.1150646e-03 9.9640441e-01 2.4609943e-03] [0. 0. 1. 0.]\n",
      "245 [4.0856390e-03 4.0361727e-05 9.9342841e-01 2.4455739e-03] [0. 0. 1. 0.]\n",
      "246 [1.8936413e-04 8.5517121e-03 9.2718518e-01 6.4073801e-02] [0. 0. 1. 0.]\n",
      "247 [1.1847862e-05 4.4112857e-03 9.9309045e-01 2.4864308e-03] [0. 0. 1. 0.]\n",
      "248 [3.2124331e-05 2.4630778e-04 9.9568427e-01 4.0372289e-03] [0. 0. 1. 0.]\n",
      "249 [1.2859718e-05 1.3854406e-03 9.9514800e-01 3.4537192e-03] [0. 0. 1. 0.]\n",
      "250 [0.1564341  0.00239233 0.8184481  0.0227254 ] [0. 0. 1. 0.]\n",
      "251 [5.7651132e-04 4.5167875e-05 9.9725014e-01 2.1282611e-03] [0. 0. 1. 0.]\n",
      "252 [1.4224854e-04 6.7459433e-03 9.7985351e-01 1.3258339e-02] [0. 0. 1. 0.]\n",
      "253 [7.1344391e-04 3.3868553e-05 9.9667764e-01 2.5750410e-03] [0. 0. 1. 0.]\n",
      "254 [1.1728041e-02 3.3161257e-04 9.7754842e-01 1.0391812e-02] [0. 0. 1. 0.]\n",
      "255 [4.6865453e-06 2.1846921e-04 9.9410939e-01 5.6674145e-03] [0. 0. 1. 0.]\n",
      "256 [1.7081898e-07 3.1097210e-04 9.9770540e-01 1.9834985e-03] [0. 0. 1. 0.]\n",
      "257 [3.4494831e-06 6.5192267e-05 9.9455845e-01 5.3729229e-03] [0. 0. 1. 0.]\n",
      "258 [9.1913546e-04 3.5197521e-05 9.9636817e-01 2.6775931e-03] [0. 0. 1. 0.]\n",
      "259 [7.1402814e-04 7.1678820e-05 9.9560815e-01 3.6062079e-03] [0. 0. 1. 0.]\n",
      "260 [3.8110163e-02 6.2259450e-04 9.4866049e-01 1.2606807e-02] [0. 0. 1. 0.]\n",
      "261 [1.3629925e-04 4.5864242e-03 9.8948509e-01 5.7921410e-03] [0. 0. 1. 0.]\n",
      "262 [1.5184369e-06 3.4582743e-05 9.9790788e-01 2.0559633e-03] [0. 0. 1. 0.]\n",
      "263 [2.1935618e-04 5.2787934e-04 9.8777628e-01 1.1476482e-02] [0. 0. 1. 0.]\n",
      "264 [1.5148326e-03 4.3942793e-05 9.9661487e-01 1.8264129e-03] [0. 0. 1. 0.]\n",
      "265 [5.5341738e-05 2.8360789e-03 9.9203414e-01 5.0744689e-03] [0. 0. 1. 0.]\n",
      "266 [6.3630880e-04 2.8027027e-05 9.9703801e-01 2.2976201e-03] [0. 0. 1. 0.]\n",
      "267 [3.5959628e-04 1.8876101e-05 9.9741131e-01 2.2102636e-03] [0. 0. 1. 0.]\n",
      "268 [1.3943744e-03 3.7601512e-05 9.9504447e-01 3.5235307e-03] [0. 0. 1. 0.]\n",
      "269 [9.8395276e-07 2.8544169e-05 9.9878579e-01 1.1846732e-03] [0. 0. 1. 0.]\n",
      "270 [5.8998576e-05 1.5138468e-03 9.9349505e-01 4.9321679e-03] [0. 0. 1. 0.]\n",
      "271 [2.3296184e-06 9.8818364e-06 9.9784374e-01 2.1439819e-03] [0. 0. 1. 0.]\n",
      "272 [3.8485067e-07 4.3651783e-03 9.8765624e-01 7.9782289e-03] [0. 0. 1. 0.]\n",
      "273 [6.4776436e-04 6.6184817e-05 9.9590069e-01 3.3853322e-03] [0. 0. 1. 0.]\n",
      "274 [1.68495978e-06 1.48923045e-05 9.98777211e-01 1.20616856e-03] [0. 0. 1. 0.]\n",
      "275 [2.7666074e-05 3.8582652e-03 9.9255908e-01 3.5550073e-03] [0. 0. 1. 0.]\n",
      "276 [3.5410695e-04 1.7392909e-05 9.9746156e-01 2.1669660e-03] [0. 0. 1. 0.]\n",
      "277 [7.8477991e-05 4.8090657e-03 9.8873585e-01 6.3765296e-03] [0. 0. 1. 0.]\n",
      "278 [1.6869114e-06 1.4853750e-05 9.9877709e-01 1.2063941e-03] [0. 0. 1. 0.]\n",
      "279 [1.1086361e-03 1.9010869e-04 9.4316125e-01 5.5540025e-02] [0. 0. 1. 0.]\n",
      "280 [3.4808196e-05 1.7782417e-03 9.9505866e-01 3.1283209e-03] [0. 0. 1. 0.]\n",
      "281 [7.6028507e-04 7.4922958e-05 9.9539953e-01 3.7653267e-03] [0. 0. 1. 0.]\n",
      "282 [1.2527070e-03 6.5104068e-05 9.9622959e-01 2.4525807e-03] [0. 0. 1. 0.]\n",
      "283 [1.8858427e-02 3.7578150e-04 9.6815699e-01 1.2608820e-02] [0. 0. 1. 0.]\n",
      "284 [1.6174864e-03 1.1675097e-04 9.5841491e-01 3.9850850e-02] [0. 0. 1. 0.]\n",
      "285 [1.27027815e-05 1.96626391e-02 9.77280140e-01 3.04458686e-03] [0. 0. 1. 0.]\n",
      "286 [1.2485140e-04 6.9673294e-03 9.8628277e-01 6.6250218e-03] [0. 0. 1. 0.]\n",
      "287 [2.9427931e-04 2.4241757e-02 9.7421074e-01 1.2532176e-03] [0. 0. 1. 0.]\n",
      "288 [8.5632582e-06 1.5733378e-02 9.8162854e-01 2.6295329e-03] [0. 0. 1. 0.]\n",
      "289 [5.128040e-04 1.857258e-02 9.794829e-01 1.431660e-03] [0. 0. 1. 0.]\n",
      "290 [2.5908164e-06 3.6297585e-03 9.9505818e-01 1.3095074e-03] [0. 0. 1. 0.]\n",
      "291 [3.5206529e-06 7.6883654e-03 9.9127370e-01 1.0343449e-03] [0. 0. 1. 0.]\n",
      "292 [0.56536436 0.03923821 0.09177338 0.303624  ] [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293 [0.34083572 0.11290836 0.20629197 0.33996385] [0. 0. 0. 1.]\n",
      "294 [0.49893323 0.0566694  0.09502774 0.3493696 ] [0. 0. 0. 1.]\n",
      "295 [0.00953071 0.01623485 0.94097304 0.03326134] [0. 0. 0. 1.]\n",
      "296 [0.09268208 0.01258247 0.16843522 0.72630024] [0. 0. 0. 1.]\n",
      "297 [0.26604655 0.02008052 0.17173399 0.54213893] [0. 0. 0. 1.]\n",
      "298 [0.20895162 0.06813651 0.08442413 0.63848776] [0. 0. 0. 1.]\n",
      "299 [0.3374478  0.06645045 0.06881021 0.5272916 ] [0. 0. 0. 1.]\n",
      "300 [0.15855925 0.02549129 0.04633135 0.7696181 ] [0. 0. 0. 1.]\n",
      "301 [0.04515143 0.00340014 0.64830685 0.30314156] [0. 0. 0. 1.]\n",
      "302 [0.02244139 0.03188162 0.06530623 0.88037074] [0. 0. 0. 1.]\n",
      "303 [0.15457264 0.17268835 0.17199264 0.5007464 ] [0. 0. 0. 1.]\n",
      "304 [0.04724836 0.08813409 0.07427023 0.7903473 ] [0. 0. 0. 1.]\n",
      "305 [0.0318852  0.06321585 0.05802988 0.84686905] [0. 0. 0. 1.]\n",
      "306 [0.01347485 0.01858431 0.02739141 0.94054943] [0. 0. 0. 1.]\n",
      "307 [0.0107647  0.03214524 0.04728654 0.90980357] [0. 0. 0. 1.]\n",
      "308 [0.0141757  0.01669572 0.03228187 0.93684673] [0. 0. 0. 1.]\n",
      "309 [0.0128248  0.01883985 0.04368006 0.9246553 ] [0. 0. 0. 1.]\n",
      "310 [0.0130934  0.03528151 0.03564445 0.9159807 ] [0. 0. 0. 1.]\n",
      "311 [0.0130784  0.03053685 0.03652821 0.91985655] [0. 0. 0. 1.]\n",
      "312 [0.02073901 0.01973463 0.03503509 0.9244912 ] [0. 0. 0. 1.]\n",
      "313 [0.01970937 0.02939942 0.03115827 0.919733  ] [0. 0. 0. 1.]\n",
      "314 [0.02531342 0.01756482 0.04759983 0.90952194] [0. 0. 0. 1.]\n",
      "315 [0.01975356 0.02016255 0.03681777 0.92326605] [0. 0. 0. 1.]\n",
      "316 [0.01743918 0.04188348 0.0377195  0.9029578 ] [0. 0. 0. 1.]\n",
      "317 [0.01683662 0.02414254 0.03050734 0.9285134 ] [0. 0. 0. 1.]\n",
      "318 [0.01592398 0.02514831 0.03176481 0.92716295] [0. 0. 0. 1.]\n",
      "319 [0.01650263 0.02709259 0.04994568 0.9064591 ] [0. 0. 0. 1.]\n",
      "320 [0.01702715 0.0253405  0.05206832 0.9055641 ] [0. 0. 0. 1.]\n",
      "321 [0.01903337 0.02603802 0.05905848 0.89587015] [0. 0. 0. 1.]\n",
      "322 [0.01649057 0.02709324 0.04998124 0.9064349 ] [0. 0. 0. 1.]\n",
      "323 [0.72855914 0.03665866 0.05815733 0.17662486] [0. 0. 0. 1.]\n",
      "324 [0.18217222 0.07343455 0.3443553  0.400038  ] [0. 0. 0. 1.]\n",
      "325 [0.7199237  0.00710714 0.02431687 0.2486524 ] [0. 0. 0. 1.]\n",
      "326 [0.17381224 0.03923622 0.578633   0.20831855] [0. 0. 0. 1.]\n",
      "327 [0.39149708 0.05717097 0.18306762 0.36826426] [0. 0. 0. 1.]\n",
      "328 [0.30818886 0.14311936 0.23277812 0.31591368] [0. 0. 0. 1.]\n",
      "329 [0.4455906  0.054718   0.18913668 0.31055483] [0. 0. 0. 1.]\n",
      "330 [0.98966837 0.00214698 0.00381033 0.00437436] [0. 0. 0. 1.]\n",
      "331 [0.84091955 0.0177148  0.13305974 0.0083059 ] [0. 0. 0. 1.]\n",
      "332 [0.41204175 0.04806625 0.2856947  0.2541973 ] [0. 0. 0. 1.]\n",
      "333 [0.14383337 0.05126144 0.7378356  0.06706951] [0. 0. 0. 1.]\n",
      "334 [0.4599132  0.05389095 0.1713954  0.31480047] [0. 0. 0. 1.]\n",
      "335 [0.7514148  0.00590131 0.06272309 0.17996079] [0. 0. 0. 1.]\n",
      "336 [0.30230087 0.12595318 0.23695599 0.33478993] [0. 0. 0. 1.]\n",
      "337 [0.5132128  0.09075844 0.16890538 0.22712335] [0. 0. 0. 1.]\n",
      "338 [0.40454057 0.05004556 0.19612357 0.34929034] [0. 0. 0. 1.]\n",
      "339 [0.45801505 0.04944089 0.14904279 0.34350124] [0. 0. 0. 1.]\n",
      "340 [0.39029402 0.12153883 0.18881841 0.29934877] [0. 0. 0. 1.]\n",
      "341 [0.9930044  0.0018251  0.00216061 0.00300995] [0. 0. 0. 1.]\n",
      "342 [0.85184747 0.0259231  0.0909016  0.03132783] [0. 0. 0. 1.]\n",
      "343 [0.33832514 0.138262   0.21580024 0.30761272] [0. 0. 0. 1.]\n",
      "344 [0.4713256  0.05745484 0.16523749 0.30598202] [0. 0. 0. 1.]\n",
      "345 [0.4316939  0.04307365 0.27173662 0.25349584] [0. 0. 0. 1.]\n",
      "346 [0.4375925  0.06336766 0.18547304 0.3135668 ] [0. 0. 0. 1.]\n",
      "347 [0.9527184  0.00109732 0.01223989 0.03394443] [0. 0. 0. 1.]\n",
      "348 [0.47384667 0.01493138 0.04457271 0.46664926] [0. 0. 0. 1.]\n",
      "349 [0.8238576  0.00414472 0.15857418 0.01342345] [0. 0. 0. 1.]\n",
      "350 [0.89712244 0.01552849 0.0280155  0.0593335 ] [0. 0. 0. 1.]\n",
      "351 [0.9746849  0.00355995 0.00597667 0.01577849] [0. 0. 0. 1.]\n",
      "352 [0.02921986 0.02175737 0.858585   0.09043775] [0. 0. 0. 1.]\n",
      "353 [0.85043263 0.02764117 0.04553578 0.07639036] [0. 0. 0. 1.]\n",
      "354 [0.393287   0.11018907 0.15379061 0.34273332] [0. 0. 0. 1.]\n",
      "355 [0.5262807  0.08176569 0.17219712 0.21975651] [0. 0. 0. 1.]\n",
      "356 [0.5764433  0.0415577  0.07074454 0.31125447] [0. 0. 0. 1.]\n",
      "357 [0.9914691  0.00187872 0.00297166 0.0036805 ] [0. 0. 0. 1.]\n",
      "358 [0.39521956 0.05447334 0.20561148 0.34469554] [0. 0. 0. 1.]\n",
      "359 [0.2961075  0.13857418 0.23831275 0.32700557] [0. 0. 0. 1.]\n",
      "360 [0.56365705 0.04597256 0.06882293 0.32154745] [0. 0. 0. 1.]\n",
      "361 [0.4007503  0.10083605 0.1748432  0.32357043] [0. 0. 0. 1.]\n",
      "362 [0.19719914 0.01335709 0.27576783 0.513676  ] [0. 0. 0. 1.]\n",
      "363 [0.42280304 0.0557151  0.13995469 0.38152713] [0. 0. 0. 1.]\n",
      "364 [0.4136867  0.08079728 0.13582137 0.36969465] [0. 0. 0. 1.]\n",
      "365 [0.43850043 0.05634684 0.13484174 0.3703109 ] [0. 0. 0. 1.]\n",
      "366 [0.93686545 0.0015116  0.04321343 0.01840951] [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_train, verbose=1)\n",
    "for i in range(len(prediction)):\n",
    "    print (i, prediction[i], categorical_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xd2335a358>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAD8CAYAAACchf2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd232d25f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[183])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "model.save('classifier_sim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9578589e-01, 1.3313984e-04, 7.4636372e-04, 3.3345884e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9957858920097351,\n",
       " 0.00013313983799889684,\n",
       " 0.0007463637157343328,\n",
       " 0.003334588371217251]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[46].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[46].tolist().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[46].tolist().index(np.max(prediction[46]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (64, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5f93c758be82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (64, 32, 3)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 32, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train[0].reshape(1,64,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(X_train[0].reshape(1,64,32,3), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].tolist().index(np.max(test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9707139e-01, 7.5927004e-05, 1.4111120e-04, 2.7115915e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970714"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].tolist().index(min(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
